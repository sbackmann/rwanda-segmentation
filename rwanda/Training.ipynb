{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c1388b4-4471-4e4a-a34a-75b6d798fff7",
   "metadata": {},
   "source": [
    "# Training\n",
    "This notebook is for reproducing the training that led to the results presented in the thesis. For this, the dataset should be saved as specified in the ``README``. Then, the following three steps are taken to enable the training:\n",
    "1. The images are split into train, validation and test set and divided into smaller image patches. The JSON files that transform the data into COCO format are generated.\n",
    "2. By specifying the experiment, for which the training shall be reproduced, the respective pre-trained COCO weights are downloaded and the configuration for the experiment is set.\n",
    "3. The dataset is built and training starts. The model is evaluated and checkpoints are created after one, respectively three epochs (depending on the experiment)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700cf10c-cfc4-4a43-80d4-51e43f065edb",
   "metadata": {},
   "source": [
    "## 0: Check Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a86183e7-895d-4043-82df-47612264d183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "1.10.1+cu111 False\n",
      "2.25.0\n",
      "11.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import mmcv\n",
    "\n",
    "ROOT_DIR = os.path.abspath(\"..\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "# Check MMDetection installation\n",
    "import mmdet\n",
    "print(mmdet.__version__) # should be 2.25.0\n",
    "\n",
    "# Check mmcv installation\n",
    "from mmcv.ops import get_compiling_cuda_version\n",
    "print(get_compiling_cuda_version()) # should be matching Pytorch's cuda version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5456c2db-663a-401d-82bd-aca0d1f629de",
   "metadata": {},
   "source": [
    "## 1: Create Dataset\n",
    "\n",
    "If the following values (except for the paths) are left unchanged, image patches and JSON files are created so that all experiments barring the one that uses image patches with a minimum scale of 150 pixels (modification \\[b\\]) can easily be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16aee7d8-5df6-496a-acc9-58cf88d9e666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting all input images into tiles of 300 x 300\n",
      "Splitting train input images into tiles of 600 x 600\n",
      "0 image patches created\n"
     ]
    }
   ],
   "source": [
    "from DataInitialization import DataInitialization\n",
    "\n",
    "# Path to the folder containing the original satellite images\n",
    "INPUT_IMG_DIR = os.path.abspath(\"./data/Training_Images_RGB\")\n",
    "# Path to the folder containing the `.shp` annotations\n",
    "ANNO_DIR = os.path.abspath(\"./data/Training_tree_polygons\")\n",
    "# Folder where train, val and test sets are created\n",
    "IMG_DIR = os.path.dirname(INPUT_IMG_DIR)\n",
    "\n",
    "# If sets are not created randomly, these indices mark the images belonging to the validation set\n",
    "# and test set\n",
    "val_images = [3, 8, 15, 16, 20, 28, 33, 35, 40, 43, 52, 60, 65, 68, 78, 79, 83, 89, 95, 98, 100, 107]\n",
    "test_images = [5, 7, 17, 18, 24, 30, 41, 45, 47, 49, 51, 59, 62, 70, 76, 77, 81, 91, 99, 104, 105, 108]\n",
    "\n",
    "# Width and height of the image patches. If a list is passed, patches of multiple sizes are created for\n",
    "# the training set. For the validation and test set, only image patches of the first size are created. \n",
    "min_width = [300, 600]\n",
    "min_height = [300, 600]\n",
    "# Which subsets to create. \n",
    "subsets = ['train', 'val', 'test']\n",
    "\n",
    "# Set up initialization of the datasets. If ``force`` is False, for each subset image patches and JSON\n",
    "# files are only created if they do not exist thus far. If set to true, additional image patches are\n",
    "# created and JSON files are regenerated, including all image patches.\n",
    "ri = DataInitialization(img_dir=INPUT_IMG_DIR,\n",
    "                          img_output_dir=IMG_DIR,\n",
    "                          subsets=subsets,\n",
    "                          force=False)\n",
    "\n",
    "# Method to create image patches\n",
    "ri.split_crop(min_width=min_width, min_height=min_height, random_split=False, val_images=val_images, test_images=test_images)\n",
    "# Method to create JSON files in COCO format\n",
    "ri.load_rwanda_data(anno_dir=ANNO_DIR)\n",
    "ri.load_rwanda_data(anno_dir=ANNO_DIR, load_prefix=\"300\", subsets=['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fabbfa-632e-4fad-a3fd-3b222e12ac84",
   "metadata": {},
   "source": [
    "## 2: Load Experiment for Training\n",
    "To reproduce the training of the different models evaluated in the thesis, their configurations can be loaded by specifying the experiment name. A list of all the experiment_names can be generated with ``print_experiments()``.\n",
    "\n",
    "\n",
    "If the widths and heights of the image patches or the load prefixes were changed from the default values in the data creation step, the paths to the JSON files in ``cfg`` might need to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e740c5-cd54-4d98-9b04-5d397afde0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskR50_-\n",
      "MaskR101_-\n",
      "MaskR50_1\n",
      "MaskR50_2\n",
      "MaskR50_3\n",
      "MaskR50_4\n",
      "MaskR50_5\n",
      "MaskR50_6\n",
      "MaskR50_7\n",
      "CascadeR50_-\n",
      "CascadeR101_-\n",
      "MaskR50_a\n",
      "MaskR50_ab\n",
      "MaskR50_ac\n",
      "MaskR50_acd\n",
      "MaskR50_acde\n",
      "CascadeR50_acd\n"
     ]
    }
   ],
   "source": [
    "from ModelInitialization import ModelInitialization\n",
    "\n",
    "ModelInitialization.print_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38515054-ab76-4c3b-9d33-7b383850d62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskR50 already downloaded.\n",
      "Loaded configuration for MaskR50_acd.\n"
     ]
    }
   ],
   "source": [
    "from ModelInitialization import ModelInitialization\n",
    "\n",
    "# Specify which training shall be reproduced.\n",
    "experiment = \"MaskR50_acd\"\n",
    "\n",
    "mi = ModelInitialization(experiment=experiment)\n",
    "model_dir = \"./pretrained_models\"\n",
    "\n",
    "# Downloads the pretrained model files if not already downloaded\n",
    "checkpoint = mi.load_pretrained_model(model_dir=model_dir)\n",
    "# Loads configuration for chosen experiment\n",
    "cfg = mi.load_config(anno_dir=ANNO_DIR, img_dir=IMG_DIR, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157cd2c9-b9ef-4eec-82de-41f6ab689629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Print Confiuration to check or change specific parameters.\n",
    "# print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a5968-a148-4f6b-a2e9-287554f4688f",
   "metadata": {},
   "source": [
    "## 3: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc21119e-cead-4eee-8c38-865f8c9b3eed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmdet.datasets import build_dataset\n",
    "from mmdet.models import build_detector\n",
    "from mmdet.apis import train_detector\n",
    "import pickle\n",
    "\n",
    "# Set seed to reproduce results\n",
    "set_random_seed(cfg.seed, deterministic=False)\n",
    "\n",
    "# Build dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the detector\n",
    "model = build_detector(cfg.model)\n",
    "\n",
    "# Add an attribute for visualization convenience\n",
    "model.CLASSES = datasets[0].CLASSES\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(os.path.abspath(cfg.work_dir))\n",
    "\n",
    "train_detector(model, datasets, cfg, distributed=False, validate=True)\n",
    "\n",
    "# After Training, save the configuration file\n",
    "with open(cfg.work_dir + '/cfg.pkl','wb') as f:\n",
    "    pickle.dump(cfg, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
